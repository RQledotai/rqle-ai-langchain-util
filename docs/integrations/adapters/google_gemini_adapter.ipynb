{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858790aa-e76c-4336-97ae-c090b38afddb",
   "metadata": {},
   "source": [
    "# Google Gemini Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fdbdde-5c38-497d-8fbd-2ad9d174bbf9",
   "metadata": {},
   "source": [
    "[Google Gemini](https://blog.google/technology/ai/google-gemini-ai/) is a family of large language models (LLMs) and associated AI applications developed by Google. They are designed to be:\n",
    "* more capable and versatile than previous Google AI models like LaMDA;\n",
    "* \"multimodal\", meaning they can understand and generate content across different formats like text, images, audio, and code\n",
    "\n",
    "Google has integrated Gemini into various products and services, including the Gemini chatbot app, Google Workspace apps, Google Search, Android, and developer tools like Google AI Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b068b1b1-90a2-4f39-bde5-4b06dba4b20d",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383bb40-69f8-480f-9d24-bb3e33ac562a",
   "metadata": {},
   "source": [
    "Your local `.env` file should include the following environment variables:\n",
    "* `GOOGLE_API_ENDPOINT` = URL to the endpoint used to serve the different models\n",
    "* `GOOGLE_API_KEY` = API key to access the service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662b4b6-09c8-4bf9-95f5-b8582d80abe2",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69359fb6-4602-4cb7-98d5-2ec6d680c9f6",
   "metadata": {},
   "source": [
    "The following library needs to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a6f6c-3b09-4ece-b04b-220c3d191b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rqle-ai-langchain-util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69dc937-bba5-4dd3-85c7-5255a897207b",
   "metadata": {},
   "source": [
    "When creating a Python script to integrate with Google Gemini service, the following dependencies are required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073c5b5-a7e7-47a1-b752-fd43fd4bf304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from rqle_ai_langchain_util import settings\n",
    "from rqle_ai_langchain_util.llms.adapters.llm_adapters import LLMAdapter\n",
    "from rqle_ai_langchain_util.llms.llm_mediator import LLMMediator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f6fc9-ef21-42a9-a5fe-7127ee862636",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd75033-9aa5-47d6-ab92-46f74538efc6",
   "metadata": {},
   "source": [
    "The integration relies on a configuration folder ([see introduction](../introduction.md)) that includes:\n",
    "* a JSON file with the configuration for the chosen LLM. \n",
    "* a text file including the prompt to be executed\n",
    "\n",
    "⚠️**Note** This also determines whether chat, completion and embedding logic should be used.\n",
    "\n",
    "Once these have been created, we can start the integration with the Google Gemini service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8031a-9307-4a5b-a76c-162d01afe88d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm_mediator = LLMMediator(LLMAdapter.GOOGLE_GEMINI, {config_folder_name})\n",
    "prompt = PromptTemplate(template=llm_mediator.prompt_template.prompt)\n",
    "llm_chain = LMChain(llm=llm_mediator.model, prompt=prompt)\n",
    "output = chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba30c541-b72d-4d77-b23e-266a1425c846",
   "metadata": {},
   "source": [
    "where `config_folder_name` is the directory where you have stored the configuration for the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cfa593-9f3c-400b-b13a-7c7300de74e4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <hr/>\n",
    "  <img src=\"../../../img/rqle_ai_logo_alt.jpeg\" alt=\"RQle.AI\" width=\"60\"/>\n",
    "  &nbsp; RQle.AI - 2024\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
