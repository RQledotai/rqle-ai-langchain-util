{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4edbe6a-c2d1-4ddd-9c6b-7401d6145675",
   "metadata": {},
   "source": [
    "# AWS Bedrock Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8d388-b3a9-4341-935b-0bf97d537629",
   "metadata": {},
   "source": [
    "[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97177c33-70c6-4461-b09f-5e164bc13076",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb504e9-fd14-4e26-bc09-7ee4cc469aff",
   "metadata": {},
   "source": [
    "Amazon Bedrock requires to be configured by following these steps:\n",
    "* Generate programmatic access key through [AWS IAM](https://us-east-1.console.aws.amazon.com/iam/home) for the user to be used\n",
    "* Download install AWS CLI for locally configuring access key / secret (see [AWS documentation](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html))\n",
    "* Run `aws configure` in a terminal to set the AWS Access key / secret (see [AWS documentation](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html))\n",
    "\n",
    "Furthermore, your local `.env.` should include the following environment variables:\n",
    "* `BWB_ENDPOINT_URL` = your endpoint for the service\n",
    "* `BWB_PROFILE_NAME` = the name of the profile as defined in `~/.aws/config` and `~/.aws/credentials` files. The default is \"default\".\n",
    "* `BWB_REGION_NAME` = the region where the service is located\n",
    "\n",
    "Once you have configured AWS Bedrock, you need to enable the different Large Language Models (LLMs) that you want to use (see [Amazon documention](https://catalog.workshops.aws/building-with-amazon-bedrock/en-US/prerequisites/bedrock-setup))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115f0d61-89a0-406c-b55c-fa2180fc3ced",
   "metadata": {},
   "source": [
    "## Known Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5574dc09-b5f6-4199-bc12-6d9ba93696e6",
   "metadata": {},
   "source": [
    "When abstracting the configuration for the Llama2 LLMS, we have encountered some issues calling the service through streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc44570-705d-4acd-921e-0e19e92d5273",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4384bd5-f175-42f3-9b0c-752644ed8bad",
   "metadata": {},
   "source": [
    "The following library needs to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065da3b8-dec7-49af-a790-66384d35085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rqle-ai-langchain-util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad29d48-c5b3-4f69-aaa9-a41ee11e100d",
   "metadata": {},
   "source": [
    "When creating a Python script to integrate with AWS Bedrock, the following dependencies are required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e63b244-8d94-4783-82a9-e315d7b6caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from rqle_ai_langchain_util import settings\n",
    "from rqle_ai_langchain_util.llms.adapters.llm_adapters import LLMAdapter\n",
    "from rqle_ai_langchain_util.llms.llm_mediator import LLMMediator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffad1d-a407-45f0-a8a5-cf7860743546",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011302cc-db77-43b2-8003-a9e94b6e8313",
   "metadata": {},
   "source": [
    "The integration relies on a configuration folder ([see introduction](../introduction.md)) that includes:\n",
    "* a JSON file with the configuration for the chosen LLM. \n",
    "* a text file including the prompt to be executed\n",
    "\n",
    "⚠️**Note** This also determines whether chat, completion or embedding logic should be used.\n",
    "\n",
    "Once these have been created, we can start the integration with the AWS Bedrock service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b583e-5783-40b1-acdb-d25431c27ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_mediator = LLMMediator(LLMAdapter.AWS_BEDROCK, {config_folder_name})\n",
    "prompt = PromptTemplate(template=llm_mediator.prompt_template.prompt)\n",
    "llm_chain = LMChain(llm=llm_mediator.model, prompt=prompt)\n",
    "output = chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac66ec8b-56eb-4570-9094-78233284155d",
   "metadata": {},
   "source": [
    "where `config_folder_name` is the directory where you have stored the configuration for the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a00a20-e226-495e-893c-963f4caf7ae7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <hr/>\n",
    "  <img src=\"../../../img/rqle_ai_logo_alt.jpeg\" alt=\"RQle.AI\" width=\"60\"/>\n",
    "  &nbsp; RQle.AI - 2024\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
